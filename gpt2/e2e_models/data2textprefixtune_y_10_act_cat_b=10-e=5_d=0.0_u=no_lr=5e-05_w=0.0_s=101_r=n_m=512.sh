python run_language_modeling.py         --output_dir=e2e_models/data2textprefixtune_y_10_act_cat_b=10-e=5_d=0.0_u=no_lr=5e-05_w=0.0_s=101_r=n_m=512         --model_type=gpt2         --model_name_or_path=gpt2-large         --tokenizer_name=gpt2-large         --per_device_train_batch_size 10         --per_device_eval_batch_size 10         --save_steps 500000         --num_train_epochs 5         --do_train         --train_data_file=./data/e2e_data/src1_test.txt         --do_eval         --line_by_line         --save_total_limit 1         --overwrite_output_dir         --task_mode data2text         --eval_data_file=./data/e2e_data/src1_valid.txt          --tuning_mode prefixtune --logging_dir e2e_models/runs\data2textprefixtune_y_10_act_cat_b=10-e=5_d=0.0_u=no_lr=5e-05_w=0.0_s=101_r=n_m=512         --train_embs no --optim_prefix yes --preseqlen 10 --prefix_mode activation --format_mode cat --gradient_accumulation_steps 1 --learning_rate 5e-05 --weight_decay 0.0 --seed 101 --disable_tqdm --mid_dim 512 --init_random no --use_dropout no --prefix_dropout 0.0 --objective_mode 1  --cache_dir \u\scr\xlisali\contrast_LM\transformers\examples\control\gpt2-medium-s3 
